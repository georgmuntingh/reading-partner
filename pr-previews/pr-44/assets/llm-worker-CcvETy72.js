(function(){"use strict";let p=null,l=null,f=!1,u=!1,i=!1,y=null;self.addEventListener("unhandledrejection",a=>{var o;const t=((o=a.reason)==null?void 0:o.message)??String(a.reason);(t.includes("Device")&&t.includes("lost")||t.includes("mapAsync"))&&self.postMessage({type:"error",error:"GPU device lost — your GPU may have run out of memory or timed out. Try a smaller model or restart the browser."})});async function b(){if(typeof navigator>"u"||!navigator.gpu)return!1;try{return await navigator.gpu.requestAdapter()!==null}catch{return!1}}async function _(a){return a==="webgpu"?"webgpu":a==="wasm"?"wasm":await b()?"webgpu":"wasm"}async function v(a){if(f)return;f=!0;const{model:t="HuggingFaceTB/SmolLM2-360M-Instruct",device:o="auto",dtype:d="q4f16",chatOptions:c=null}=a;y=c;try{self.postMessage({type:"loading",progress:{status:"Loading transformers.js library..."}});const n=await import("https://cdn.jsdelivr.net/npm/@huggingface/transformers@3");n.env.allowLocalModels=!1;const s=await _(o);self.postMessage({type:"loading",progress:{status:`Loading LLM model (${s}, ${d})...`}}),l=await n.AutoTokenizer.from_pretrained(t,{progress_callback:e=>{e.status==="progress"&&self.postMessage({type:"loading",progress:{status:`Downloading ${e.file}`,file:e.file,loaded:e.loaded,total:e.total,progress:e.progress}})}}),p=await n.AutoModelForCausalLM.from_pretrained(t,{device:s,dtype:d,progress_callback:e=>{e.status==="progress"&&self.postMessage({type:"loading",progress:{status:`Downloading ${e.file}`,file:e.file,loaded:e.loaded,total:e.total,progress:e.progress}})}}),f=!1,self.postMessage({type:"ready",info:{model:t,device:s,dtype:d}})}catch(n){f=!1;let s=n.message??String(n);s.includes("Aborted")?s=`Model session failed to start (dtype "${d}" may not be supported on this GPU). Try a different model.`:(s.includes("Device")&&s.includes("lost")||s.includes("mapAsync"))&&(s="GPU device lost while loading the model — your GPU may have run out of memory. Try a smaller model."),self.postMessage({type:"error",error:s})}}async function T(a){if(!p||!l){self.postMessage({type:"error",error:"Model not loaded"});return}if(u){self.postMessage({type:"error",error:"Already generating"});return}u=!0,i=!1;const{messages:t,max_new_tokens:o=512,temperature:d=.7,do_sample:c=!0,top_p:n=.9,repetition_penalty:s=1.1}=a;try{const e=l.apply_chat_template(t,{add_generation_prompt:!0,return_tensor:!1,...y||{}}),{Tensor:r}=await import("https://cdn.jsdelivr.net/npm/@huggingface/transformers@3"),U=new r("int64",BigInt64Array.from(e.map(BigInt)),[1,e.length]),P=new r("int64",new BigInt64Array(e.length).fill(1n),[1,e.length]);let L=[],g="",h=0;const I=await p.generate({input_ids:U,attention_mask:P,max_new_tokens:o,temperature:c?d:1,do_sample:c,top_p:c?n:void 0,repetition_penalty:s,streamer:{put(M){if(i)return;const m=Array.from(M.flat()).slice(e.length).slice(h);if(h+=m.length,m.length===0)return;L.push(...m);const w=l.decode(m,{skip_special_tokens:!0});w&&(g+=w,self.postMessage({type:"token",token:w}))},end(){}}});if(u=!1,i)self.postMessage({type:"aborted",text:g});else{const k=I.tolist()[0].slice(e.length);g=l.decode(k,{skip_special_tokens:!0}),self.postMessage({type:"complete",text:g})}}catch(e){if(u=!1,i)self.postMessage({type:"aborted",text:""});else{let r=e.message??String(e);(r.includes("Device")&&r.includes("lost")||r.includes("mapAsync"))&&(r="GPU device lost during generation — your GPU may have run out of memory or timed out. Try a smaller model."),self.postMessage({type:"error",error:r})}}}function A(){i=!0}function G(){p=null,l=null,u=!1,i=!1,y=null,self.postMessage({type:"unloaded"})}self.onmessage=async a=>{const{type:t,...o}=a.data;switch(t){case"load":await v(o);break;case"generate":await T(o);break;case"abort":A();break;case"unload":G();break;default:console.warn(`[llm-worker] Unknown message type: ${t}`)}}})();
